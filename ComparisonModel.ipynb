{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214b72e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leclouxj\\CSC 2611 - AI Tools\\FinalProject\\COM2611-Final-Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2266e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/spam.csv'\n",
    "data = pd.read_csv(data_path, encoding='Windows-1252')\n",
    "data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "data.columns = ['label', 'email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b079f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leclouxj\\CSC 2611 - AI Tools\\FinalProject\\COM2611-Final-Project\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7468281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\"ham\" : 0, \"spam\" : 1}\n",
    "labels = data[\"label\"].map(encoding)\n",
    "X_main, X_test, y_main, y_test = train_test_split(data['email'], labels, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3058f4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e111d209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(X_val),   truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(X_test),  truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d199e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = list(labels)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        item = {k: torch.tensor(v[index]) for k, v in self.encodings.items()} \n",
    "        item['labels'] = torch.tensor(self.labels[index], dtype = torch.long)\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da4fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(train_encodings, y_train)\n",
    "test_dataset = SpamDataset(test_encodings, y_test)\n",
    "val_dataset = SpamDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defa6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d8dd471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce6d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leclouxj\\AppData\\Local\\Temp\\ipykernel_7436\\842634874.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must run on Rosie. Will take 6 hours otherwise\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8bf1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './results'\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir, fix_mistral_regex=True)\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f65cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leclouxj\\AppData\\Local\\Temp\\ipykernel_7436\\2885242191.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  loaded_trainer = Trainer(model=loaded_model, tokenizer=loaded_tokenizer)\n",
      "c:\\Users\\leclouxj\\CSC 2611 - AI Tools\\FinalProject\\COM2611-Final-Project\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9964125560538116, 'f1': 0.9879518072289156, 'precision': 0.9879518072289156, 'recall': 0.9879518072289156}\n"
     ]
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "loaded_trainer = Trainer(model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "pred_output = loaded_trainer.predict(test_dataset)\n",
    "print(compute_metrics(pred_output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
